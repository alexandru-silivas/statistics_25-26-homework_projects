<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Homework 6 — Online Mean & Variance (Derivations & Implementation)</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <!-- NAVBAR -->
  <header class="navbar">
    <nav>
      <ul class="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="homework_1.html">HMWK 1</a></li>
        <li><a href="homework_2.html">HMWK 2</a></li>
        <li><a href="homework_3.html">HMWK 3</a></li>
        <li><a href="homework_4.html">HMWK 4</a></li>
        <li><a href="homework_5.html">HMWK 5</a></li>
        <li><a href="homework_6.html" class="active">HMWK 6</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <div class="main-content">
      <h1>Homework 6</h1>
      <hr class="section-separator">
      <h2>Online Computation of Mean and Variance</h2>

      <p>
        In many data-driven and real-time systems, data arrive continuously and it is inefficient or even
        impossible to store all previous samples to recompute statistics each time.
        The goal of this exercise is to <strong>derive simple recurrence relationships</strong>
        for the arithmetic mean and the variance and to <strong>implement online algorithms</strong>
        that update these quantities incrementally as new data become available.
      </p>

      <h3>1. The Arithmetic Mean (Measure of Location)</h3>
      <p>
        The <strong>arithmetic mean</strong> summarizes the "central tendency" of a dataset.
        For a sequence of values \(x_1, x_2, \ldots, x_n\), the mean after
        \(n\) observations is defined as:
      </p>
      <pre><code class="math">μₙ = (1/n) · Σᵢ₌₁ⁿ xᵢ</code></pre>
      <p>
        Suppose we already know the mean after \(n-1\) observations, \(μₙ₋₁\).  
        When a new observation \(xₙ\) arrives, the updated mean can be computed without
        summing everything again:
      </p>
      <pre><code class="math">
μₙ = ( (n−1)·μₙ₋₁ + xₙ ) / n
    = μₙ₋₁ + (xₙ − μₙ₋₁)/n
      </code></pre>
      <p>
        This recurrence allows the mean to be updated in constant time and constant memory.
        It is the foundation for all online algorithms.
      </p>

      <h3>2. The Variance (Measure of Dispersion)</h3>
      <p>
        While the mean tells us where the data tend to cluster (location),
        it says nothing about how widely spread they are.  
        The <strong>variance</strong> quantifies this <em>dispersion</em>:
      </p>
      <pre><code class="math">
σ² = (1/n) · Σᵢ₌₁ⁿ (xᵢ − μₙ)²
      </code></pre>
      <p>
        A low variance indicates that values are close to the mean, while a high variance
        shows that data points are more dispersed.
        Measures of location and dispersion are complementary:
        <ul>
          <li>The mean alone cannot describe the variability of data.</li>
          <li>The variance alone cannot describe where the data are centered.</li>
        </ul>
        Together they give a more complete statistical picture of a dataset.
      </p>

      <h3>3. Derivation of the Online Variance (Welford’s Algorithm)</h3>
      <p>
        Define \(M2_n = \sum_{i=1}^{n} (x_i - μ_n)^2\).
        When a new value \(x_n\) is added:
      </p>
      <pre><code class="math">
δ  = xₙ − μₙ₋₁
μₙ = μₙ₋₁ + δ / n
M2ₙ = M2ₙ₋₁ + δ · (xₙ − μₙ)
      </code></pre>
      <p>
        The key insight is that \(M2\) accumulates the sum of squared deviations
        <em>without</em> needing all previous values.  
        The population variance is \(σ² = M2ₙ / n\),
        and the unbiased sample variance is \(s² = M2ₙ / (n−1)\).
      </p>
      <p>
        Welford’s formulation avoids the numerical instability of the naïve
        two-pass algorithm (compute mean, then subtract), which suffers from
        <strong>catastrophic cancellation</strong> when data are large or tightly clustered.
      </p>

      <h3>4. Implementation and Demonstration</h3>
      <p>
        The script below uses the recurrences above to update mean and variance online.
        Each click adds a random value (most small, some large outliers) and immediately
        updates both statistics. The chart shows how the mean and variance evolve
        as more data points are observed.
      </p>

      <section id="online-demo">
        <canvas id="onlineStatsChart"></canvas>

        <div style="margin-top:14px;" class="rsa-row">
          <button type="button" onclick="addRandomValue()">Add Random Value</button>
          <button type="button" onclick="resetData()">Reset</button>
        </div>

        <p id="statsDisplay">No data yet.</p>
      </section>

      <h3>5. Code Walkthrough</h3>
      <p>
        The online algorithm is implemented in JavaScript as follows:
      </p>
      <pre><code>
// Initialization
let count = 0, mean = 0, M2 = 0;

// Update step (for each new x)
count++;
const delta = x - mean;
mean += delta / count;
M2 += delta * (x - mean);

const variance = M2 / count;
      </code></pre>
      <p>
        Each update uses only the previous mean, the current count, and the new value.
        This keeps computation <strong>O(1)</strong> per data point
        and avoids storing past observations.
      </p>
      <p>
        The chart is generated using <strong>Chart.js</strong>,
        dynamically plotting the sequence of means and variances.
        It provides a visual demonstration that as the number of samples increases,
        both quantities converge to stable values — illustrating
        the <em>law of large numbers</em> in action.
      </p>

      <hr>
      <a href="homework_5.html" class="back-button">← Back to Homework 5</a>
    </div>
  </main>

  <footer class="site-footer">
    © 2025 Alexandru Silivas — ID 2252622 —
    <a href="mailto:silivas.2252622@studenti.uniroma1.it">silivas.2252622@studenti.uniroma1.it</a>
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="mean_variance_online.js"></script>
</body>
</html>
